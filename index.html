<!DOCTYPE html>

<html>
    <head>
        <meta charset="utf-8">
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">

        <meta name="description" content="Homepage of Ke Tan">
        <meta name="author" content="Ke Tan">

	    <title>Ke Tan (谭可)</title>

        <!-- Bootflat CSS -->
        <link rel="stylesheet" href="css/bootflat.css">

        <!-- Bootstrap Core CSS -->
        <link href="css/bootstrap.min.css" rel="stylesheet">

        <!-- Custom CSS -->
        <link href="css/index.css" rel="stylesheet">

        <!-- Custom Fonts -->
        <link href="font-awesome-4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
        <link href="http://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic" rel="stylesheet" type="text/css">
        <script src="https://kit.fontawesome.com/6169794306.js" crossorigin="anonymous"></script>
    </head>


    <body>
        <div id="top" style="position: relative; top: -100px;"></div>
        <header id="header_navbar">
		    <nav class="navbar navbar-default navbar-fixed-top fadeIn" role="navigation">
                <div>
                    <div class="navbar-header">
					    <button type="button" class="navbar-toggle collapsed" id="button_navbar" data-toggle="collapse" data-target="#dropdown-box-1">
						    <span class="sr-only">Toggle navigation</span>
						    <span class="icon-bar"></span>
						    <span class="icon-bar"></span>
						    <span class="icon-bar"></span>
					    </button>
				    </div>

    		        <div class="collapse navbar-collapse navbar-right" id="dropdown-box-1">
	    			    <ul class="nav navbar-nav">
		    			    <li><a href="#home_target">Home</a></li>
			    			<li><a href="#bio_target">Bio</a></li>
					    	<li><a href="#manuscripts_target">Manuscripts</a></li>
		    				<li><a href="#publications_target">Publications</a></li>
                            <li><a href="#software_target">Software</a></li>
                            <li><a href="#presentations_target">Presentations</a></li>
		    				<li><a href="#working_target">Working</a></li>
                            <li><a href="#acknowledgments_target">Acknowledgments</a></li>
				    	</ul>
			        </div>
                </div>
		    </nav>
	    </header>


        <div id="container">
            <div id="home_target" style="position: relative; top: -50px;"></div>
	        <div id="home">
                <div id="photo"> <a><img src="img/photo.png" width="230" height="230"/></a> </div>

    	        <div id="intro">
        	        <div id="name"> <h1>Ke Tan (谭可)</h1> </div>
                    <div>
        	            <p>
                            Ph.D. candidate <br/>
            	            <a href="https://cse.osu.edu/" target="_blank">Department of Computer Science and Engineering</a> <br/>
            	            <a href="https://www.osu.edu/" target="_blank">The Ohio State University</a> <br/>
            	            Advisor: <a href="http://web.cse.ohio-state.edu/~dwang/" target="_blank">DeLiang Wang</a> <br/>
                            Lab: <a href="http://www.cse.ohio-state.edu/pnl" target="_blank">Perception and Neurodynamics Laboratory (PNL)</a> <br/>
                            Email: <a>tan.650@osu.edu</a>
                        </p>
                    </div>
                    <div id="contact">
                        <ul class="list-inline">
                            <li><a href="./doc/Curriculum Vitae.pdf" target="_blank"><i class="fas fa-file"></i> CV</a></li>
                            <li><a href="https://www.linkedin.com/in/ke-tan/" target="_blank"><i class="fab fa-linkedin"></i> LinkedIn</a></li>
                            <li><a href="https://scholar.google.com/citations?hl=en&user=bAzfOgoAAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i> Google Scholar</a></li>
                            <li><a href="https://www.researchgate.net/profile/Ke_Tan6" target="_blank"><i class="fab fa-researchgate"></i> ResearchGate</a></li>
                        </ul>
                    </div>
                </div>
            </div>

            <div id="clear"></div>

            <div id="bio_target" style="position: relative; top: -50px;"></div>
            <div id="bio">
    	        <h2><a name="bio">Bio</a></h2>

                <p>I am currently a sixth-year (final-year) Ph.D. student in CSE at OSU, advised by Prof. <a href="http://web.cse.ohio-state.edu/~dwang/" target="_blank">DeLiang Wang</a>. Before studying at OSU, I received the bachelor degree in electronic information engineering from <a href="http://en.ustc.edu.cn/" target="_blank">University of Science and Technology of China</a> (USTC) in 2015.

                <p>My research focuses on <b><i>speech enhancement</i></b>, <b><i>speech separation</i></b>, <b><i>speech dereverberation</i></b>, and <b><i>deep learning</i></b>. I am also interested in <b><i>microphone array processing</i></b>, <b><i>audio-visual speech enhancement and separation</i></b>, <b><i>acoustic echo cancellation</i></b> and <b><i>keyword spotting</i></b>. I serve as a reviewer for IEEE/ACM Transactions on Audio, Speech, and Language Processing, IEEE Journal of Selected Topics in Signal Processing, IEEE Signal Processing Letters, IEEE Communications Letters, The Journal of the Acoustical Society of America, and Speech Communication.</p>
            </div>

            <div id="manuscripts_target" style="position: relative; top: -50px;"></div>
            <div id="manuscripts">
    	        <h2><a name="manuscripts">Manuscripts</a></h2>
                <p align="justify">
                    [2] <b>K. Tan</b> and D. L. Wang, <b>"Towards Model Compression for Deep Learning Based Speech Enhancement"</b>, in submission to <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing (<b>IEEE/ACM TASLP</b>)</i>, under review. <br>
                </p>

                <p align="justify">
                    [1] <b>K. Tan</b>, X. Zhang, and D. L. Wang, <b>"Deep Learning Based Real-Time Speech Enhancement for Dual-Microphone Mobile Phones"</b>, in submission to <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing (<b>IEEE/ACM TASLP</b>)</i>, under review. <br>
                </p>
            </div>

            <div id="publications_target" style="position: relative; top: -50px;"></div>
            <div id="publications">
    	        <h2><a name="publications">Publications</a></h2>
                <h4><a name="journal"><i>Journal Articles</i></a></h4>
                <p align="justify">
                    [6] E. W. Healy, <b>K. Tan</b>, E. M. Johnson, and D. L. Wang, <b>"An Effectively Causal Deep Learning Algorithm to Increase Intelligibility in Novel Noises for Hearing-Impaired Listeners"</b>, in <i>Journal of the Acoustical Society of America (<b>JASA</b>)</i>, in press, 2021. <br>
                </p>

                <p align="justify">
                    [5] <b>K. Tan</b>, B. Xu, A. Kumar, E. Nachmani, and Y. Adi, <b>"SAGRNN: Self-Attentive Gated RNN for Binaural Speaker Separation with Interaural Cue Preservation"</b>, in <i>IEEE Signal Processing Letters (<b>IEEE SPL</b>)</i>, vol. 28, pp. 26-30, 2021. <br>
                    <a class="label label-success" href="doc/papers/TXKNA.spl21.pdf" target="_blank">Paper</a>
                    &nbsp;
                    <a class="label label-info" href="doc/papers/bib/tan2021sagrnn.txt" target="_blank">BibTeX</a>
                    &nbsp;
                    <a class="demos label" href="https://jupiterethan.github.io/sagrnn.github.io/" target="_blank">Demos</a>
                </p>

                <p align="justify">
                    [4] <b>K. Tan</b>, Y. Xu, S.-X. Zhang, M. Yu, and D. Yu, <b>"Audio-Visual Speech Separation and Dereverberation with a Two-Stage Multimodal Network"</b>, in <i>IEEE Journal of Selected Topics in Signal Processing (<b>IEEE JSTSP</b>)</i>, vol. 14, pp. 542-553, 2020. <br>
                    <a class="label label-success" href="doc/papers/TXZYY.jstsp20.pdf" target="_blank">Paper</a>
                    &nbsp;
                    <a class="label label-info" href="doc/papers/bib/tan2020audio.txt" target="_blank">BibTeX</a>
                    &nbsp;
                    <a class="demos label" href="https://jupiterethan.github.io/av-enh.github.io/" target="_blank">Demos</a>
                </p>

                <p align="justify">
                    [3] <b>K. Tan</b> and D. L. Wang, <b>"Learning Complex Spectral Mapping with Gated Convolutional Recurrent Networks for Monaural Speech Enhancement"</b>, in <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing (<b>IEEE/ACM TASLP</b>)</i>, vol. 28, pp. 380-390, 2020. <br>
                    <a class="label label-success" href="doc/papers/TW.taslp20.pdf" target="_blank">Paper</a>
                    &nbsp;
                    <a class="label label-info" href="doc/papers/bib/tan2020learning.txt" target="_blank">BibTeX</a>
                    &nbsp;
                    <a class="code label" href="https://github.com/JupiterEthan/GCRN-complex" target="_blank">Code</a>
                </p>

                <p align="justify">
                    [2] P. Wang, <b>K. Tan</b> and D. L. Wang, <b>"Bridging the Gap Between Monaural Speech Enhancement and Recognition with Distortion-Independent Acoustic Modeling"</b>, in <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing (<b>IEEE/ACM TASLP</b>)</i>, vol. 28, pp. 39-48, 2020. <br>
                    <a class="label label-success" href="doc/papers/WTW.taslp20.pdf" target="_blank">Paper</a>
                    &nbsp;
                    <a class="label label-info" href="doc/papers/bib/wang2020bridging.txt" target="_blank">BibTeX</a>
                </p>

                <p align="justify">
                    [1] <b>K. Tan</b>, J. Chen, and D. L. Wang, <b>"Gated Residual Networks with Dilated Convolutions for Monaural Speech Enhancement"</b>, in <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing (<b>IEEE/ACM TASLP</b>)</i>, vol. 27, pp. 189-198, 2019. <br>
                    <a class="label label-success" href="doc/papers/TCW.taslp19.pdf" target="_blank">Paper</a>
                    &nbsp;
                    <a class="label label-info" href="doc/papers/bib/tan2019gated.txt" target="_blank">BibTeX</a>
                </p>

                <br>
                <h4><a name="conference"><i>Conference Papers</i></a></h4>
                <p align="justify">
                    [12] <b>K. Tan</b>, X. Zhang, and D. L. Wang, <b>"Real-Time Speech Enhancement for Mobile Communication Based on Dual-Channel Complex Spectral Mapping"</b>, in <i>IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP</b>)</i>, 2021. <br>
                    <a class="label label-info" href="doc/papers/bib/tan2021real.txt" target="_blank">BibTeX</a>
                </p>

                <p align="justify">
                    [11] <b>K. Tan</b> and D. L. Wang, <b>"Compressing Deep Neural Networks for Efficient Speech Enhancement"</b>, in <i>IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP</b>)</i>, 2021. <br>
                    <a class="label label-info" href="doc/papers/bib/tan2021compressing.txt" target="_blank">BibTeX</a>
                </p>

                <p align="justify">
                    [10] <b>K. Tan</b> and D. L. Wang, <b>"Improving Robustness of Deep Learning Based Monaural Speech Enhancement Against Processing Artifacts"</b>, in <i>IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP</b>)</i>, pp. 6914-6918, 2020. <br>
                    <a class="label label-success" href="doc/papers/TW.icassp20.pdf" target="_blank">Paper</a>
                    &nbsp;
                    <a class="label label-info" href="doc/papers/bib/tan2020improving.txt" target="_blank">BibTeX</a>
                </p>

                <p align="justify">
                    [9] H. Zhang, <b>K. Tan</b> and D. L. Wang, <b>"Deep Learning for Joint Acoustic Echo and Noise Cancellation with Nonlinear Distortions"</b>, in <i>the 20th Annual Conference of the International Speech Communication Association (<b>INTERSPEECH</b>)</i>, pp. 4255-4259, 2019. <br>
                    <a class="label label-success" href="doc/papers/ZTW.interspeech19.pdf" target="_blank">Paper</a>
                    &nbsp;
                    <a class="label label-info" href="doc/papers/bib/zhang2019deep.txt" target="_blank">BibTeX</a>
                </p>

                <p align="justify">
                    [8] P. Wang, <b>K. Tan</b> and D. L. Wang, <b>"Bridging the Gap Between Monaural Speech Enhancement and Recognition with Distortion-Independent Acoustic Modeling"</b>, in <i>the 20th Annual Conference of the International Speech Communication Association (<b>INTERSPEECH</b>)</i>, pp. 471-475, 2019. <br>
                    <a class="label label-success" href="doc/papers/WTW.interspeech19.pdf" target="_blank">Paper</a>
                    &nbsp;
                    <a class="label label-info" href="doc/papers/bib/wang2019bridging.txt" target="_blank">BibTeX</a>
                </p>

                <p align="justify">
                    [7] <b>K. Tan</b> and D. L. Wang, <b>"Complex Spectral Mapping with a Convolutional Recurrent Network for Monaural Speech Enhancement"</b>, in <i>IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP</b>)</i>, pp. 6865-6869, 2019. <br>
                    <a class="label label-success" href="doc/papers/TW.icassp19.pdf" target="_blank">Paper</a>
                    &nbsp;
                    <a class="label label-info" href="doc/papers/bib/tan2019complex.txt" target="_blank">BibTeX</a>
                </p>

                <p align="justify">
                    [6] <b>K. Tan</b>, X. Zhang, and D. L. Wang, <b>"Real-Time Speech Enhancement Using an Efficient Convolutional Recurrent Network for Dual-Microphone Mobile Phones in Close-Talk Scenarios"</b>, in <i>IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP</b>)</i>, pp. 5751-5755, 2019. <br>
                    <a class="label label-success" href="doc/papers/TZW.icassp19.pdf" target="_blank">Paper</a>
                    &nbsp;
                    <a class="label label-info" href="doc/papers/bib/tan2019real.txt" target="_blank">BibTeX</a>
                </p>

                <p align="justify">
                    [5] Z.-Q. Wang, <b>K. Tan</b>, and D. L. Wang, <b>"Deep Learning Based Phase Reconstruction for Speaker Separation: A Trigonometric Perspective"</b>, in <i>IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP</b>)</i>, pp. 71-75, 2019. <br>
                    <a class="label label-success" href="doc/papers/WTW.icassp19.pdf" target="_blank">Paper</a>
                    &nbsp;
                    <a class="label label-info" href="doc/papers/bib/wang2019deep.txt" target="_blank">BibTeX</a>
                </p>

                <p align="justify">
                    [4] <b>K. Tan</b> and D. L. Wang, <b>"A Convolutional Recurrent Neural Network for Real-Time Speech Enhancement"</b>, in <i>the 19th Annual Conference of the International Speech Communication Association (<b>INTERSPEECH</b>)</i>, pp. 3229-3233, 2018. <br>
                    <a class="label label-success" href="doc/papers/Tan-Wang1.interspeech18.pdf" target="_blank">Paper</a>
                    &nbsp;
                    <a class="label label-info" href="doc/papers/bib/tan2018convolutional.txt" target="_blank">BibTeX</a>
                    &nbsp;
                    <a class="code label" href="https://github.com/JupiterEthan/CRN-causal" target="_blank">Code</a>
                </p>

                <p align="justify">
                    [3] <b>K. Tan</b> and D. L. Wang, <b>"A Two-Stage Approach to Noisy Cochannel Speech Separation with Gated Residual Networks"</b>, in <i>the 19th Annual Conference of the International Speech Communication Association (<b>INTERSPEECH</b>)</i>, pp. 3484-3488, 2018. <br>
                    <a class="label label-success" href="doc/papers/Tan-Wang2.interspeech18.pdf" target="_blank">Paper</a>
                    &nbsp;
                    <a class="label label-info" href="doc/papers/bib/tan2018two.txt" target="_blank">BibTeX</a>
                </p>

                <p align="justify">
                    [2] <b>K. Tan</b>, J. Chen, and D. L. Wang, <b>"Gated Residual Networks with Dilated Convolutions for Supervised Speech Separation"</b>, in <i>IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP</b>)</i>, pp. 21-25, 2018. <br>
                    <a class="label label-success" href="doc/papers/TCW.icassp18.pdf" target="_blank">Paper</a>
                    &nbsp;
                    <a class="label label-info" href="doc/papers/bib/tan2018gated.txt" target="_blank">BibTeX</a>
                </p>

                <p align="justify">
                    [1] S. Zhu, <b>K. Tan</b>, X. Zhang, Z. Liu and B. Liu, <b>"MICROST: A Mixed Approach for Heart Rate Monitoring During Intensive Physical Exercise Using Wrist-Type PPG Signals"</b>, in <i>the 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (<b>EMBC</b>)</i>, pp. 2347-2350, 2015. <br>
                    <a class="label label-success" href="doc/papers/MICROST.pdf" target="_blank">Paper</a>
                    &nbsp;
                    <a class="label label-info" href="doc/papers/bib/zhu2015microst.txt" target="_blank">BibTeX</a>
                </p>

            </div>

            <div id="software_target" style="position: relative; top: -50px;"></div>
            <div id="software">
    	        <h2><a name="software">Software</a></h2>
                <ul>
                    <li>GCRN for monaural speech enhancement: <a href="doc/papers/TW.taslp20.pdf" target="_blank">2020 Tan-Wang paper</a>, and <a href="https://github.com/JupiterEthan/GCRN-complex" target="_blank">PyTorch code on GitHub</a>.</li>
                    <li>CRN for monaural speech enhancement: <a href="doc/papers/Tan-Wang1.interspeech18.pdf" target="_blank">2018 Tan-Wang paper</a>, and <a href="https://github.com/JupiterEthan/CRN-causal" target="_blank">PyTorch code on GitHub</a>.</li>
                </ul>
            </div>

            <div id="presentations_target" style="position: relative; top: -50px;"></div>
            <div id="presentations">
    	        <h2><a name="presentations">Presentations</a></h2>

                <p align="justify">
                    [5] <a class="label label-success" href="doc/presentations/robustness_icassp2020" target="_blank">Slides</a> <i>Improving Robustness of Deep Learning Based Monaural Speech Enhancement Against Processing Artifacts</i>, IEEE ICASSP (virtually due to COVID-19 pandemic), Barcelona, Spain, May 2020.
                </p>


                <p align="justify">
                    [4] <a class="label label-success" href="doc/presentations/complex_icassp2019.pdf" target="_blank">Poster</a> <i>Complex Spectral Mapping with a Convolutional Recurrent Network for Monaural Speech Enhancement</i>, IEEE ICASSP, Brighton, United Kingdom, May 2019.
                </p>

                <p align="justify">
                    [3] <a class="label label-success" href="doc/presentations/dual_icassp2019.pdf" target="_blank">Slides</a> <i>Real-Time Speech Enhancement Using an Efficient Convolutional Recurrent Network for Dual-Microphone Mobile Phones in Close-Talk Scenarios</i>, IEEE ICASSP, Brighton, United Kingdom, May 2019.
                </p>

                <p align="justify">
                    [2] <a class="label label-success" href="doc/presentations/phase_icassp2019.pdf" target="_blank">Slides</a> <i>Deep Learning Based Phase Reconstruction for Speaker Separation: A Trigonometric Perspective</i>, IEEE ICASSP, Brighton, United Kingdom, May 2019.
                </p>

                <p align="justify">
                    [1] <a class="label label-success" href="doc/presentations/GRN_icassp2018.pdf" target="_blank">Slides</a> <i>Gated Residual Networks with Dilated Convolutions for Supervised Speech Separation</i>, IEEE ICASSP, Calgary, Alberta, Canada, Apr. 2018.
                </p>
            </div>

            <div id="working_target" style="position: relative; top: -50px;"></div>
	        <div id="working">
            	<h2><a name="working">Working</a></h2>
		        <p>Jan. 2017 - present, <i>Graduate Research Associate</i> in <a href="http://web.cse.ohio-state.edu/pnl/" target="_blank">Perception and Neurodynamics Laboratory (PNL)</a> at OSU, Columbus, OH, United States</p>
                <p>May 2020 - Aug. 2020, <i>Research Intern</i> at <a href="https://research.fb.com/category/augmented-reality-virtual-reality/" target="_blank">Facebook Reality Labs</a> (formerly <a href="https://www.oculus.com/research/" target="_blank">Oculus Research</a>), Redmond, WA, United States</p>
                <p>May 2019 - Aug. 2019, <i>Research Intern</i> at <a href="https://ai.tencent.com/ailab/en/index" target="_blank">Tencent AI Lab</a>, Bellevue, WA, United States</p>
                <p>May. 2018 - Aug. 2018, <i>Research Intern</i> at <a href="http://kitt.ai/" target="_blank">KITT.AI group - Baidu DuerOS</a>, Bellevue, WA, United States</p>
            </div>

            <div id="acknowledgments_target" style="position: relative; top: -50px;"></div>
            <div id="acknowledgments">
            	<h2><a name="acknowledgments">Acknowledgments</a></h2>
                <a href="http://en.ustc.edu.cn/" target="_blank">
                	<img src="img/ustc.png" width="110" height="110"/>
                </a>
                <a href="https://www.osu.edu/" target="_blank">
                	<img src="img/osu.png" width="110" height="110"/>
                </a>
                <a href="http://research.baidu.com/" target="_blank">
                	<img src="img/baidu.png" width="110" height="110"/>
                </a>
                <a href="https://ai.tencent.com/ailab/en/index" target="_blank">
                	<img src="img/tencent_ai_lab.png" height="110"/>
                </a>
                <a href="https://research.fb.com/" target="_blank">
                	<img src="img/facebook.png" height="110"/>
                </a>
            </div>

        </div>


        <div id="footer"><i>Designed by <span style="color:#FFF">Ke Tan</span> on <span style="color:#FFF">05/11/2015</span>. Last updated on <span style="color:#FFF">04/28/2021</span>.</i></div>

        <div class="wow fadeIn" data-wow-delay="0.4s">
	        <div class="btn-circle-scroll">
			    <a href="#top" class="btn-circle">
				    <i class="fa fa-angle-double-up"></i>
				</a>
			</div>
		</div>

        <script src="js/jquery-3.2.1.min.js"></script>
        <script src="js/bootstrap.min.js"></script>
    	<script src="js/jquery.easing.min.js"></script>
	    <script src="js/jquery.scrollTo.js"></script>
        <script src="js/wow.min.js"></script>

        <script type="text/javascript">
            $(function(){
                $("ul li a").click(function(){
                    var hr = $(this).attr("href");
                    var anh = $(hr).offset().top;
                    $("html,body").stop().animate({scrollTop:anh}, 1000);
                })
            })
        </script>

        <script type="text/javascript">
            (function($) {
        	    new WOW().init();

	            jQuery(window).load(function() {
		            jQuery("#preloader").delay(100).fadeOut("slow");
		            jQuery("#load").delay(100).fadeOut("slow");
	            });

            	//jQuery to collapse the navbar on scroll
	            $(window).scroll(function() {
	        	    if ($(".navbar").offset().top > 50) {
			            $(".navbar-fixed-top").addClass("top-nav-collapse");
		            } else {
			            $(".navbar-fixed-top").removeClass("top-nav-collapse");
	            	}
            	});
            })(jQuery);
        </script>

        <script type="text/javascript">
            $(document).on("scroll",function(){
	            if($(document).scrollTop() > 50){
		            $(".navbar").stop().animate({
                        opacity: '0.7'
		            });
	            }else{
		            $(".navbar").stop().animate({
                        opacity:'1.0'
		            });
	            }
            });
        </script>

        <script type="text/javascript">
        	//jQuery for page scrolling feature - requires jQuery Easing plugin
	        $(function() {
		        $('.btn-circle-scroll a').bind('click', function(event) {
			        var $anchor = $(this);
			        $('html, body').stop().animate({
			           scrollTop: $($anchor.attr('href')).offset().top
			           }, 1500, 'easeInOutExpo');
		    	    event.preventDefault();
		        });
	        });
        </script>

        <!-- Email -->
        <script type="text/javascript">
            var a = "tan.650";
            var b = "osu.edu";
            document.getElementById("osu-email").href = "mailto:" + a + "@" + b;
        </script>

        <!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=qNIolzwkkPaDtv2ywjbBzS1SIC2QUqX_jQHm_wC97nk&cl=ffffff&w=a"></script> -->
        <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=N_fnzVPTxT9LnxhA1Kyo7UsLjhDJUcDGulQVkGx8xAM'></script>

    </body>
</html>
